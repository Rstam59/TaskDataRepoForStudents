{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNnPB6SQZgTizCtTAFkheR4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rstam59/TaskDataRepoForStudents/blob/main/Date_conversion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKx5XGm4D9H2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from datetime import date\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, TextVectorization\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "\n",
        "MONTHS = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\",\n",
        "          \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n",
        "\n",
        "def random_dates(n_dates):\n",
        "    min_date = date(1000, 1, 1).toordinal()\n",
        "    max_date = date(9999, 12, 31).toordinal()\n",
        "    ordinals = np.random.randint(max_date - min_date, size=n_dates) + min_date\n",
        "    dates = [date.fromordinal(ordinal) for ordinal in ordinals]\n",
        "    x = [MONTHS[d.month - 1] + \" \" + d.strftime(\"%d, %Y\") for d in dates]\n",
        "    y = [d.strftime(\"%Y-%m-%d\") for d in dates]\n",
        "    return x, y\n",
        "\n",
        "np.random.seed(42)\n",
        "x_texts, y_texts = random_dates(10000)\n",
        "\n",
        "# Add \"startofseq\" and \"endofseq\" markers\n",
        "decoder_inputs_texts = [f\"startofseq {text}\" for text in y_texts]\n",
        "decoder_targets_texts = [f\"{text} endofseq\" for text in y_texts]\n",
        "\n",
        "\n",
        "max_input_len = max(len(txt) for txt in x_texts)\n",
        "max_decoder_input_len = max(len(txt) for txt in decoder_inputs_texts)\n",
        "max_decoder_target_len = max(len(txt) for txt in decoder_targets_texts)\n",
        "\n",
        "input_vectorizer = TextVectorization(\n",
        "    output_mode='int',\n",
        "    output_sequence_length=max_input_len,\n",
        "    split='character',\n",
        "    standardize=None\n",
        ")\n",
        "decoder_vectorizer = TextVectorization(\n",
        "    output_mode='int',\n",
        "    output_sequence_length=max(max_decoder_input_len, max_decoder_target_len),\n",
        "    split='character',\n",
        "    standardize=None\n",
        ")\n",
        "\n",
        "# Adapt vectorizers\n",
        "input_vectorizer.adapt(x_texts)\n",
        "decoder_vectorizer.adapt(decoder_inputs_texts + decoder_targets_texts)\n",
        "\n",
        "# Vectorize\n",
        "encoder_input = input_vectorizer(tf.constant(x_texts))\n",
        "decoder_input = decoder_vectorizer(tf.constant(decoder_inputs_texts))\n",
        "decoder_target = decoder_vectorizer(tf.constant(decoder_targets_texts))\n",
        "\n",
        "# Get vocab size\n",
        "input_vocab_size = len(input_vectorizer.get_vocabulary())\n",
        "target_vocab_size = len(decoder_vectorizer.get_vocabulary())\n",
        "\n",
        "# Convert target to one-hot for teacher forcing\n",
        "decoder_target = tf.one_hot(decoder_target, depth=target_vocab_size)\n",
        "\n",
        "\n",
        "latent_dim = 256\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(None,), name=\"encoder_inputs\")\n",
        "x = Embedding(input_vocab_size, latent_dim, mask_zero=True)(encoder_inputs)\n",
        "encoder_outputs, state_h, state_c = LSTM(latent_dim, return_state=True)(x)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = Input(shape=(None,), name=\"decoder_inputs\")\n",
        "x = Embedding(target_vocab_size, latent_dim, mask_zero=True)(decoder_inputs)\n",
        "x = LSTM(latent_dim, return_state=True)(x, initial_state=encoder_states)\n",
        "decoder_outputs = Dense(target_vocab_size, activation=\"softmax\")(x)\n",
        "\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "\n",
        "\n",
        "model.fit(\n",
        "    [encoder_input, decoder_input],\n",
        "    decoder_target,\n",
        "    batch_size=64,\n",
        "    epochs=10,\n",
        "    validation_split=0.2\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "89hjADKMEgmT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}