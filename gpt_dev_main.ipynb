{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOZohx1Gb2rCJtRn0wT3cJ4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rstam59/TaskDataRepoForStudents/blob/main/gpt_dev_main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOd8__YrX--f",
        "outputId": "1d498515-dd60-4be4-f3c4-c5e35c6a17b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-20 14:22:37--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-11-20 14:22:37 (47.6 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"input.txt\", 'r', encoding = 'utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "Hgz3SIYXZIuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwdgJKF6ZhvV",
        "outputId": "d4db08b4-2d0a-4d82-e6c3-92b42c128b91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cprC464bZh55",
        "outputId": "afe351e8-633d-485d-97d8-400f0322479a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uYDsTMEZk1d",
        "outputId": "0362e719-ef03-4658-939e-c8c6a431013d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i: ch for ch, i in stoi.items()}\n",
        "\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "print(encode('Heyy there'))\n",
        "print(decode(encode('Heyy there')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rBzUC_oZr1F",
        "outputId": "36438b70-c271-4c16-aad7-339cda1057e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[20, 43, 63, 63, 1, 58, 46, 43, 56, 43]\n",
            "Heyy there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "data = torch.tensor(encode(text), dtype = torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBjkZzRLbXjE",
        "outputId": "802631d2-b51b-48d1-b5ee-2401c89420be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
            "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
            "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
            "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
            "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
            "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
            "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
            "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
            "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
            "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
            "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
            "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
            "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
            "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
            "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
            "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
            "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
            "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
            "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
            "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
            "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
            "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
            "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
            "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
            "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
            "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
            "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
            "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
            "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
            "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
            "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
            "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
            "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
            "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
            "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
            "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
            "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
            "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
            "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
            "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
            "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
            "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
            "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
            "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
            "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
            "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
            "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(len(data) * 0.9)\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "zM5bM1SWhJm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "train_data[:block_size + 1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEEEcQ9KjWPR",
        "outputId": "c76ef30a-e252-4fd5-c4c3-70e0903c0385"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size + 1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t + 1]\n",
        "    target = y[t]\n",
        "    print(f'When input is {context} the target: {target}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YmOckccjc1m",
        "outputId": "482026de-c9d3-4c3d-ff3e-e69d1b06204b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "When input is tensor([18]) the target: 47\n",
            "When input is tensor([18, 47]) the target: 56\n",
            "When input is tensor([18, 47, 56]) the target: 57\n",
            "When input is tensor([18, 47, 56, 57]) the target: 58\n",
            "When input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
            "When input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mnDQMVHWuRHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size): # batch dimension\n",
        "    for t in range(block_size): # time dimension\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljJ0WvIckveP",
        "outputId": "1240c42d-4832-474e-8b3c-ab0d20df2f04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
            "----\n",
            "when input is [24] the target: 43\n",
            "when input is [24, 43] the target: 58\n",
            "when input is [24, 43, 58] the target: 5\n",
            "when input is [24, 43, 58, 5] the target: 57\n",
            "when input is [24, 43, 58, 5, 57] the target: 1\n",
            "when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
            "when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
            "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
            "when input is [44] the target: 53\n",
            "when input is [44, 53] the target: 56\n",
            "when input is [44, 53, 56] the target: 1\n",
            "when input is [44, 53, 56, 1] the target: 58\n",
            "when input is [44, 53, 56, 1, 58] the target: 46\n",
            "when input is [44, 53, 56, 1, 58, 46] the target: 39\n",
            "when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
            "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
            "when input is [52] the target: 58\n",
            "when input is [52, 58] the target: 1\n",
            "when input is [52, 58, 1] the target: 58\n",
            "when input is [52, 58, 1, 58] the target: 46\n",
            "when input is [52, 58, 1, 58, 46] the target: 39\n",
            "when input is [52, 58, 1, 58, 46, 39] the target: 58\n",
            "when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
            "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
            "when input is [25] the target: 17\n",
            "when input is [25, 17] the target: 27\n",
            "when input is [25, 17, 27] the target: 10\n",
            "when input is [25, 17, 27, 10] the target: 0\n",
            "when input is [25, 17, 27, 10, 0] the target: 21\n",
            "when input is [25, 17, 27, 10, 0, 21] the target: 1\n",
            "when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
            "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Cydi1kg5lhxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C) 4, 8, 65\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIezhebtrefh",
        "outputId": "dd6f27ff-d9a1-4547-86d1-989481d681f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 65])\n",
            "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(m.parameters(), lr = 1e-3)"
      ],
      "metadata": {
        "id": "UeXslfX55vT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "for steps in range(10000):\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none = True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_f0OhvKs_d3n",
        "outputId": "d8077f6c-9a6d-4f7e-b93c-c44a94ca8c2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5727508068084717\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=300)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKsGurktDIcx",
        "outputId": "28761c6e-9756-4ab6-954c-377be6189047"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iyoteng h hasbe pave pirance\n",
            "Rie hicomyonthar's\n",
            "Plinseard ith henoure wounonthioneir thondy, y heltieiengerofo'dsssit ey\n",
            "KIN d pe wither vouprrouthercc.\n",
            "hathe; d!\n",
            "My hind tt hinig t ouchos tes; st yo hind wotte grotonear 'so it t jod weancotha:\n",
            "h hay.JUCle n prids, r loncave w hollular s O:\n",
            "HIs; ht \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#The mathematical trick in self-attention"
      ],
      "metadata": {
        "id": "CY1zwZ9QDflS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "B, T, C = 4, 8, 2\n",
        "x = torch.randn(B, T, C)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3JX87cyMAdR",
        "outputId": "bca9fa9a-378a-4ea4-beeb-2db74d90b757"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xbow = torch.zeros((B, T, C))\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev = x[b, :t + 1]\n",
        "        xbow[b, t] = torch.mean(xprev, 0)"
      ],
      "metadata": {
        "id": "egC7SPIgMU10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCCAw7YBM6cD",
        "outputId": "2ae31ddb-0260-475f-dfcc-dab26bf3efab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.1808, -0.0700],\n",
              "        [-0.3596, -0.9152],\n",
              "        [ 0.6258,  0.0255],\n",
              "        [ 0.9545,  0.0643],\n",
              "        [ 0.3612,  1.1679],\n",
              "        [-1.3499, -0.5102],\n",
              "        [ 0.2360, -0.2398],\n",
              "        [-0.9211,  1.5433]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xbow[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_HcS3xSNCN5",
        "outputId": "62edd6a9-c2f7-4dcf-f55a-fc4fab705272"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.1808, -0.0700],\n",
              "        [-0.0894, -0.4926],\n",
              "        [ 0.1490, -0.3199],\n",
              "        [ 0.3504, -0.2238],\n",
              "        [ 0.3525,  0.0545],\n",
              "        [ 0.0688, -0.0396],\n",
              "        [ 0.0927, -0.0682],\n",
              "        [-0.0341,  0.1332]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#version 2\n",
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3, 3))\n",
        "a = a / torch.sum(a, 1, keepdim = True)\n",
        "b = torch.randint(0, 10, (3, 2)).float()\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('--')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('--')\n",
        "print('c=')\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcJtpNX7ND3u",
        "outputId": "9145da4c-7e65-452d-b342-15cccaf1df7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "--\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "--\n",
            "c=\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.tril(torch.ones(3, 3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmrJbfGkP3Ra",
        "outputId": "7cef1a8f-9ff6-44e8-a1b7-632297e7d3f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0.],\n",
              "        [1., 1., 0.],\n",
              "        [1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei = wei / wei.sum(1, keepdim = True)\n",
        "wei"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8Su0TRJQWtZ",
        "outputId": "356dc622-e1e8-41d0-b41d-76673c91e0a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
              "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
              "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xbow2 = wei @ x"
      ],
      "metadata": {
        "id": "z_VgRizMluNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xbow2.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtrM_YnjcTq9",
        "outputId": "a04abf95-a0ad-4854-f165-677fd1b2eb54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#version 3\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T, T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim = -1)\n",
        "xbow3 = wei @ x\n",
        "torch.allclose(xbow2, xbow3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSbv9AnNnb69",
        "outputId": "ec6e9576-585f-44e0-be22-b6518f63ff31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 4. self-attention\n",
        "B, T, C = 4, 8, 32\n",
        "x = torch.randn(B, T, C)\n",
        "\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias = False)\n",
        "query = nn.Linear(C, head_size, bias = False)\n",
        "value = nn.Linear(C, head_size, bias = False)\n",
        "k = key(x)\n",
        "q = query(x)\n",
        "wei = q @ k.transpose(-2, -1)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "# wei = torch.zeros((T, T))\n",
        "# wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim = -1)\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "# out = wei @ x\n",
        "\n"
      ],
      "metadata": {
        "id": "NzeanVaJo0J3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notes:\n"
      ],
      "metadata": {
        "id": "sO3h1MFoUZSS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Attention is a communication mechanism. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
        "# There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
        "# Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
        "# In an \"encoder\" attention block just delete the single line that does masking with tril, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
        "# \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
        "# \"Scaled\" attention additional divides wei by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
      ],
      "metadata": {
        "id": "wlFdGYinifZn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k = torch.randn(B, T, head_size)\n",
        "q = torch.randn(B, T, head_size)\n",
        "wei = q @ k.transpose(-2, -1) * head_size ** -0.5"
      ],
      "metadata": {
        "id": "Rcs0r4nNigVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2B0XqHTgjgmL",
        "outputId": "0632f81b-df97-4b1a-ac74-c0babec2f1b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0447)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pL4Wh7wwjhu-",
        "outputId": "a9f1d465-a38b-488e-c2c8-5189c1eabc68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.9216)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omkjVyjSji1U",
        "outputId": "7c2618e4-e259-4e97-fd7d-bfe82f329060"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.9243)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim = -1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tWeiCzfjku6",
        "outputId": "f4b8fc8d-ce57-4029-a620-e2a85cb61b53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]) * 8, dim = -1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Yv6e1m3kCik",
        "outputId": "8eee0064-e05d-422a-e935-d99fd791922f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BatchNorm1D:\n",
        "    def __init__(self, dim, eps = 1e-5, momentum = 0.1):\n",
        "        self.eps = eps\n",
        "\n",
        "        self.gamma = torch.ones(dim)\n",
        "        self.beta = torch.zeros(dim)\n",
        "\n",
        "\n",
        "    def __call__(self, x):\n",
        "\n",
        "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n",
        "        self.out = self.gamma * xhat + self.beta\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        def parameters(self):\n",
        "            return [self.gamma, self.beta]\n",
        "\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "module = BatchNorm1D(100)\n",
        "x = torch.randn(32, 100)\n",
        "x = module(x)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKfubhAikS6H",
        "outputId": "54c2f5e8-3c0c-4fd3-f816-783f99b160f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[:, 0].mean(), x[:, 0].std()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PB8oiD6IiWB",
        "outputId": "8fa3bfa4-1cc6-4620-a1d3-308b0bd23018"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.1469), tensor(0.8803))"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[0, :].mean(), x[0, :].std()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVlHa44DI_BX",
        "outputId": "5bbb9f83-ea0a-4d3f-8b84-080a0fe828df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(-9.5367e-09), tensor(1.0000))"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "with open(\"/content/WhatsApp Chat with Fatima.txt\", 'r', encoding = 'utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 64 # how many independent sequences will we process in parallel?\n",
        "block_size = 256 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input of size (batch, time-step, channels)\n",
        "        # output of size (batch, time-step, head size)\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,hs)\n",
        "        q = self.query(x) # (B,T,hs)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,hs)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = GPTLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n",
        "#open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "6V52NNQ4JQPQ",
        "outputId": "794cca93-2e63-44fa-f7d0-bf660123e5b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.884285 M parameters\n",
            "step 0: train loss 5.2909, val loss 5.3064\n",
            "step 500: train loss 0.9077, val loss 1.0629\n",
            "step 1000: train loss 0.7260, val loss 0.9186\n",
            "step 1500: train loss 0.5861, val loss 0.8775\n",
            "step 2000: train loss 0.4757, val loss 0.8876\n",
            "step 2500: train loss 0.3709, val loss 0.9381\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-b6bdceef0bae>\u001b[0m in \u001b[0;36m<cell line: 208>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;31m# sample a batch of data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;31m# evaluate the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-b6bdceef0bae>\u001b[0m in \u001b[0;36mget_batch\u001b[0;34m(split)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mblock_size\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mblock_size\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Default title text\n",
        "print(decode(m.generate(context, max_new_tokens=5000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InamCGOGVSYB",
        "outputId": "8add9c15-f5fb-4b5e-8b03-924c53beda21",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "64/10/2024, 17:02 - Rustam Alizada: Bt) onda N gün baMaraqsisan?😁\n",
            "09/10/2024, 17:17 - Rustam Alizada: Saat 8 okaydirsə sən  danın biləcəb elərsin x🫢3576zgün  gəlməcəm😁\n",
            "09/10/2024, 18:15 - Narmin Huseynzada: <Media omitted>\n",
            "09/10/2024, 17:52 - Rustam Alizada: Təsəli olamaam başım. Mediropa yansanı baxarım\n",
            "09/10/2024, 17:56 - Narmin Huseynzada: Yo sarann\n",
            "09/10/2024, 19::16 - Narmin Huseynzada: Ola ghayaa ozleltionumirem\n",
            "09/10/2024, 23:17 - Rustam Alizada: Tamamam basa dogure delerem\n",
            "09/10/2024, 23:18 - Rustam Alizada: Yenaremə\n",
            "09/10/2024, 21:18 - Narmin Huseynzada: Yo dərsaq\n",
            "09/10/2024, 23:18 - Rustam Alizada: TamamaLadşa bal. İşinda <This message was edited>\n",
            "09/10/2024, 23:12 - Rustam Alizada: Narmin bu😁\n",
            "09/10/2024, 23:53 - Rustam Alizada: Can, günu?\n",
            "09/10/2024, 23:5ı - Rustam Alizada: Hahahahah b qədər mər bu dəyək işdüşürdm baş, yaxü bir yerəklərdəm pis hisss etəməylız barapis e\n",
            "09/10/2024, 23:54 - Rustam Alizada: <Media omitted>\n",
            "09/10/2024, 23:54 - Narmin Huseynzada: 😂😂😂😂\n",
            "09/10/2024, 23:56 - Narmin Huseynzada: Demərsə kaş yeləş bilər pi deya yaxşıdı birm yaxşıdı yağıməsajd\n",
            "09/10/2024, 23:54 - Narmin Huseynzada: Bəsle deyəsad ən yarısagırahat eləyirsan hə\n",
            "09/10/2024, 23:54 - Narmin Huseynzada: Həəladı\n",
            "09/10/2024, 23:54 - Narmin Huseynzada: Belə idiraz fatişləyirdim. Ordama amma azca\n",
            "09/10/2024, 23:47 - Rustam Alizada: nevinde kimencirc\n",
            "09/10/2024, 23:31 - Rustam Alizada: dogurdan?A Narmin?😂 ok\n",
            "09/10/2024, 23:41 - Narmin Huseynzada: İndə ersəncə dil\n",
            "09/10/2024, 23:41 - Rustam Alizada: Dus?\n",
            "09/10/2024, 23:31 - Rustam Alizada: Tamamam ba😂sa düş\n",
            "09/10/2024, 23:41 - Rustam Alizada: <Hediahis message\n",
            "09/10/2024, 23:41 - Rustam Alizada: <Media omitted>\n",
            "09/10/2024, 23:41 - Rustam Alizada: Can, çətin gün idi? <This messsage was ededited>\n",
            "09/10/2024, 23:41 - Rustam Alizada: İşdə <This messsage was edited>\n",
            "09/10/2024, 23:41 - Rustam Alizada: Can😁\n",
            "09/10/2024, 23:41 - Rustam Alizada: Balım, 09 - deli mu telle baxim faxsi\n",
            "09/10/2024, 23:41 - Rustam Alizada: Amma Ve he bir gunll\n",
            "09/10/2024, 23:41 - Narmin Huseynzada: 🤣🤣🤣🤣🤣🤣aaaa indi çalır glər bağlar\n",
            "09/10/2024, 23:41 - Narmin Huseynzada: Amma sadelebsah or deyersi biazentir din doğunu isamed edireyi olursan deyem\n",
            "09/10/2024, 23:32 - Rustam Alizada: Hahaha faha yaxsida cf\n",
            "09/10/2024, 23:32 - Rustam Alizada: Usten pan tekayedirler fio tolsen de buŞirsən\n",
            "09/10/2024, 23:32 - Rustam Alizada: Amma yaxsi heqribkalirem jucazireməd\n",
            "09/10/2024, 23:38 - Narmin Huseynzada: Hayyasiz idam nana neseçir qolaram debelelik isteb olur olsahacam. 1 0/10/2024, 00:39 - Rustam Alizada: Selfistediye mence debasa guresan dusen ki\n",
            "09/10/2024, 00:39 - Narmin Huseynzada: <Media omitted>\n",
            "09/10/2024, 09:31 - Rustam Alizada: <Media omitted>\n",
            "09/10/2024, 19:10 - Rustam Alizada: <Media omitted>\n",
            "09/10/2024, 20:41 - Rustam Alizada: Dersən ollarsan başa düşməyə fikləşlərimin evrləri görmə umən də bu qidayıram in gəllər pis ediyirsən\n",
            "09/10/202024, 22:05 - Narmin Huseynzada: <Media omitted>\n",
            "09/10/2024, 22:07 - Narmin Huseynzada: Bun fərqində olman manı xosajt eyacam.\n",
            "09/10/2024, 22:07 - Rustam Alizada: Yo dərsi oldəladı\n",
            "09/10/2024, 22:18 - Rustam Alizada: Ca😂\n",
            "09/10/2024, 22:18 - Narmin Huseynzada: İndi jarə eləxbəcəm\n",
            "09/10/2024, 22:08 - Rustam Alizada: Yox hap insanları olara bağucla olar işləyirəm nzgordan işsən😂 başeydiı09/10/2024, 22:11 - Rustam Alizada: AHnəə ona görə görə etməcə\n",
            "09/10/2024, 22:07 - Rustam Alizada: yaxşıdı wwaynlrdosi yişlərəm. Aar<Media omitted>\n",
            "09/10/2024, 22:18 - Rustam Alizada: Balir mene beledeye demek golsar deyersen balaridi. Amma arad\n",
            "09/10/2024, 22:14 - Rustam Alizada: Helede hamid yamin dexin?09/10/2024, 22:10 - Rustam Alizada: dogunu olmur bele decem\n",
            "09/10/2024, 22:10 - Rustam Alizada: Gözlə konu😁\n",
            "09/10/2024, 22:10 - Narmin Huseynzada: Ene bilərik deeC😄\n",
            "09/10/2024, 23:10 - Rustam Alizada: Parad gozel yorluşməni olsuzun ke yencer boxlu\n",
            "09/10/202024, 22:10 - Rustam Alizada: And ha yox hələdə\n",
            "09/10/2024, 23:11 - Rustam Alizada: Yedərəm\n",
            "09/10/2024, 23:12 - Narmin Huseynzada: Prməsəni deyə şayə qaşmaladan bilərsən\n",
            "09/10/2024, 22:12 - Rustam Alizada: Deyemək\n",
            "09/10/2024, 22:12 - Rustam Alizada: Əslində evab heçə kişdim o zayox qolarşləyəa bağım vaxtı\n",
            "09/10/2024, 22:15 - Rustam Alizada: <Media omitted>\n",
            "09/10/2024, 22:16 - Rustam Alizada: Dogurdan istedane issage\n",
            "09/10/2024, 22:47 - Rustam Alizada: Akinces qabaşire di100 idi tamm hemissag w .\n",
            "09/10/2024, 23:40 - Rustam Alizada: Cann😁\n",
            "09/10/2024, 23:41 - Rustam Alizada: Heçeqox issen hirler\n",
            "09/10/2024, 23:41 - Rustam Alizada: He xanarxatirla yidirm 1200 iddi deyesen\n",
            "09/10/2024, 23:41 - Narmin Huseynzada: Niyə\n",
            "09/10/2024, 23:42 - Rustam Alizada: Post qartiyesən. Hiqan ne derlə bilk\n",
            "09/10/2024, 23:42 - Narmin Huseynzada: Yo dərsindəncə olma❤️ reləşəyik\n",
            "09/10/2024, 23:54 - Rustam Alizada: Tamam Esəncləyi bursənladə onu\n",
            "09/10/202024, 23:55 - Rustam Alizada: Dehset eledi men olanab xasi cavaxt olda\n",
            "09/10/2024, 23:55 - Narmin Huseynzada: 😂😂😂\n",
            "09/10/2024, 23:56 - Rustam Alizada: Cann, inaniram bunese desen dese kisleyersen deye senr ki\n",
            "09/10/20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "batch_size = 64\n",
        "block_size = 256\n",
        "max_iters = 3000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "torch.manual_seed(42)\n",
        "\n",
        "\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i: ch for ch, i in stoi.items()}\n",
        "\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "data = torch.tensor(encode(text), dtype = torch.long)\n",
        "\n",
        "\n",
        "n = int(len(data) * 0.9)\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias = False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias = False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias = False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2, -1) * C** -0.5\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim = -1)\n",
        "        #perform the weighted aggregation of the values\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim = -1)\n",
        "        out = self.proj(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd,n_embd),\n",
        "            nn.Dropout(dropout)\n",
        "\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head = n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T= idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        token_emb = self.token_embedding_table(idx)# (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) #(T, C)\n",
        "        x = token_emb + pos_emb #(B, T, C)\n",
        "        x = self.blocks(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            #crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "\n",
        "model = BigramLanguageModel(vocab_size)\n",
        "m = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        # print(f'step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}')\n",
        "        print(f'step {iter}: loss {losses}')\n",
        "\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none = True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "context = torch.zeros((1, 1), dtype = torch.long, device = device)\n",
        "print(decode(m.generate(context, max_new_tokens = 500)[0].tolist()))"
      ],
      "metadata": {
        "id": "GdDlBTADfmyl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eefb8e08-e81b-41f1-8512-8fbdbf86196e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: loss {'train': tensor(4.5580), 'val': tensor(4.5616)}\n",
            "step 500: loss {'train': tensor(2.0287), 'val': tensor(2.0966)}\n",
            "step 1000: loss {'train': tensor(1.5969), 'val': tensor(1.7872)}\n",
            "step 1500: loss {'train': tensor(1.4313), 'val': tensor(1.6614)}\n",
            "step 2000: loss {'train': tensor(1.3312), 'val': tensor(1.5981)}\n",
            "step 2500: loss {'train': tensor(1.2565), 'val': tensor(1.5583)}\n",
            "\n",
            "\n",
            "Second Watchman:\n",
            "Do you not what with usuffle aboard?\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "So much, like a langer liark amazon,\n",
            "And free her back now.\n",
            "\n",
            "First Servingman:\n",
            "No; but, some by gentleman company, and by\n",
            "the roinely know on me, my books are at home\n",
            "assisted: weary at the still be bones,\n",
            "Marquise thankful happy burnaturef,\n",
            "If they in-an be made admicated,--then,\n",
            "The gods hope sloop'd, and strettled state,\n",
            "And be man'd thou dear me by growtry and thee\n",
            "To better to the manoberted it.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "Then, \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(context, max_new_tokens = 5000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H350MAhJ89yx",
        "outputId": "73007364-43d2-4870-84c3-68adf90f24e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "VOLUMNUS:\n",
            "First, though you not.\n",
            "\n",
            "AUFIDIUS:\n",
            "My liege, sir: were you stout to this Antium.\n",
            "\n",
            "Childram:\n",
            "There is not all.\n",
            "\n",
            "Citizen:\n",
            "No, to god my liege.\n",
            "\n",
            "CORIOLANUS:\n",
            "I wish my genel.\n",
            "\n",
            "Messenger:\n",
            "Methinks, I make be well-mean,\n",
            "In they past choler-fool, is one both been,\n",
            "Ne'er-kind, and 'tis a word eye a griever stay,\n",
            "And never bid in elmity roar we't,\n",
            "O, name I married in the enemies.\n",
            "\n",
            "BENVOLIO:\n",
            "You are lies to know your fores, stink.\n",
            "Farewell, preparace did by your mother; and I\n",
            "Reneal to be more so thank of your grace\n",
            "To faunt of Plantia? or thou back it not\n",
            "Shall be profisent man: she is cope; O, fariar,\n",
            "And we hear for your steep's state,\n",
            "Nothing to be many absence elsed main:\n",
            "Unless and good Norfolk, death, pretty of heaven:\n",
            "Upon the body joce of the your days\n",
            "Doth the clacknes, but she's cheeks and be fled wital?\n",
            "O, if excellent my late in apparement\n",
            "As I beseech in phince my toor mind\n",
            "Have Montague by the worn he uncasting:\n",
            "To himself to demised man my least,\n",
            "Anven all Clopio your queens' lost!\n",
            "\n",
            "KING RICHARD II:\n",
            "O that's not too grew morn, as I, boy!\n",
            "As attoored as succed gnering what men:\n",
            "So many break left again.\n",
            "\n",
            "First Watchman:\n",
            "Nay, I, no yet it, it is cre word,\n",
            "To excorp it twinles by our cusable Benture.\n",
            "\n",
            "Third Citizen:\n",
            "There is on preducented and by beauty,\n",
            "And there tho my belottedy act for bear,\n",
            "So setzain against my siciet of the leave\n",
            "Would most brew then than inummently\n",
            "Acconsent it, and soon in alone.\n",
            "\n",
            "CORIOLONUS:\n",
            "What is of that?\n",
            "\n",
            "SICINIUS:\n",
            "\n",
            "CORIOLANUS:\n",
            "It's not so?\n",
            "\n",
            "CORIOLANUS:\n",
            "Not well hear.\n",
            "\n",
            "VOLUMNIA:\n",
            "And so then hour of a king of it.\n",
            "\n",
            "BRUTUS:\n",
            "Whatereful so seems ten, that sings you have\n",
            "Pretter than when my old he either the ill-be?\n",
            "\n",
            "COMINIUS:\n",
            "He was now we? not or no near to just axecution,\n",
            "That seek the glory woman the partace's loyalts\n",
            "And patience cannot sin's death earth peace timp.\n",
            "Unshamed incensention of the kings?\n",
            "\n",
            "Shepherd:\n",
            "At he's the duke?\n",
            "\n",
            "Servant:\n",
            "That goodness read--\n",
            "\n",
            "COMINIUS:\n",
            "Well, you tell you know this man. Come, you\n",
            "could not be dumber; for by rehedigness\n",
            "Than sinift me warm of my ggardenet.\n",
            "\n",
            "COMINIUS:\n",
            "Then ears are is a-day,\n",
            "Or an no budle it words comfort-shinining-breath.\n",
            "Gine of grave, bones all sweet Ant wave's grove good\n",
            "To more goist hind.\n",
            "\n",
            "SICINIUS:\n",
            "If you bare furgened teach thee\n",
            "But heaven the heaven bodemies, become of,\n",
            "Some you more beautied to fool; upon nose\n",
            "Then of upon to-name! on, no set--the uspiecy\n",
            "To betterneD IIsbednessed.\n",
            "\n",
            "BISHecause, and Joine! why is the wiser?\n",
            "\n",
            "Muster:\n",
            "Not villains and that expesse must be prettion,\n",
            "Still inklingly show a shooks to-day are\n",
            "Stigaler-shings. That I should not man's very modes\n",
            "are thind thinks: I to wake my brother,\n",
            "And shall not yong to be his grief!\n",
            "\n",
            "FLORIZEL:\n",
            "O Menenius,\n",
            "If you be your pient of my father.\n",
            "\n",
            "PERDITA:\n",
            "By your unbeards, Jove--\n",
            "\n",
            "CAMILLO:\n",
            "So you have been it and uncudains trug assence,\n",
            "Shall be comes and I, to streng to grieve,\n",
            "That they make Petries is rememember.\n",
            "\n",
            "ESCALUS:\n",
            "Why, is it all not my own the king, ne'er gentlemen?\n",
            "\n",
            "ESCALUS:\n",
            "Ay, sir.' knee you hope.\n",
            "A good many soon! say, to that was sport him:\n",
            "I hear he him a boson to-morrow in thine;\n",
            "For he hand swoot an holy tarts than he charge,\n",
            "his any obile our fail: but then or his defend\n",
            "with to open our steel mislike pow and next\n",
            "Of tears it, stand the infections of his rot--\n",
            "\n",
            "KING HENRY VI:\n",
            "To God't year gry out would play we all.\n",
            "\n",
            "ROMEO:\n",
            "Come.\n",
            "If it myself cause, dramench, be very king\n",
            "I should decept me of thy words and pallet,\n",
            "If one such sdied with these says apothects,\n",
            "If you see me doth attainted to expreptuated\n",
            "Made you stow to make up and brothech in arms,\n",
            "Go of my king, son your men, good--\n",
            "\n",
            "KING HENRY VI:\n",
            "O Backet, good my sudden laments,\n",
            "If Lord Sarry other Saint Ely firsts,\n",
            "Attoniganst King Henry's gifts back with heaven's\n",
            "In large against that so all that us.\n",
            "\n",
            "EDWARD:\n",
            "Let me towe unfingers; go, my builder, pronexince.\n",
            "\n",
            "WARWICK:\n",
            "Not words better it;\n",
            "For I do that fid great to be plant in.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "Yea, my lord,\n",
            "Will storm to you beat of York and good?\n",
            "They shall know your leave is very cklead:\n",
            "And yet our grace banife at one soul.\n",
            "\n",
            "Groman:\n",
            "But how Veris, shall allotabe that they move be\n",
            "play seek, loping give us bough and hadless vances,\n",
            "She dance like the search or me by a can\n",
            "I best, might be calm the\n",
            "tell to bear drown to oake thee, bloody all\n",
            "thy insumments to their shadows thy hangments,\n",
            "Making them my seeming coalled in o'erwisele,\n",
            "And temptorious for myself.\n",
            "Make have the pass, the with the day\n",
            "Is prevail'd content. Is change myself,\n",
            "Against thou mean mean unto me; and\n",
            "But not joy that lives proud to-my gensal pure\n",
            "Frave I shall inferincementance gentlewoman\n",
            "Of foreigness bones,--in fine enemeral-hole\n",
            "So many hours, thy son, though and man probate\n",
            "me impose me at modestadements,\n",
            "And not prepareared upon all the old coward\n",
            "And foe, behold tear thee and your epent:\n",
            "Master for 'twere are by nicks of a moon\n",
            "The safe-blooding was shamed his view,\n",
            "And such same we bade, sir, so that it consul.\n",
            "\n",
            "ANGELO:\n",
            "Why say you will de\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# File paths\n",
        "input_file = \"/content/WhatsApp Chat with Fatima.txt\"\n",
        "output_file = \"/content/output.txt\"\n",
        "\n",
        "# Read the file\n",
        "with open(input_file, \"r\", encoding=\"utf-8\") as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Remove date and time using regex\n",
        "# Pattern explanation:\n",
        "# \\d{2}/\\d{2}/\\d{4}, \\d{2}:\\d{2} - Matches date and time in the format \"dd/mm/yyyy, hh:mm\"\n",
        "cleaned_text = re.sub(r\"\\d{2}/\\d{2}/\\d{4}, \\d{2}:\\d{2} - \", \"\", text)\n",
        "\n",
        "# Save the cleaned text\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
        "    file.write(cleaned_text)\n",
        "\n",
        "print(\"Dates and times removed. Cleaned text saved to\", output_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-NS7wsG9CoR",
        "outputId": "16e47e82-c22e-4a33-885a-268289a90716"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dates and times removed. Cleaned text saved to /content/output.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_text[:1000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "AtrmozEL9gfm",
        "outputId": "87339903-a01e-4f0f-acb8-0829606f7424"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Messages and calls are end-to-end encrypted. No one outside of this chat, not even WhatsApp, can read or listen to them. Tap to learn more.\\nFatima: Kaaaaaaart\\nRustam Alizada: Fatimaa, sevmedin oyunu diyesen?🥺\\nRustam Alizada: Mende darixdim acigi orda\\nRustam Alizada: 4098584450348394\\nFatima: Çox sistemsiz idi ee, kim gəldi vururdu topa çaşırdım 😀\\nFatima: Gəldi?\\nRustam Alizada: He gordumm. O qara paltarli gelib arxadan topuna girmeyi hele\\nFatima: 😁\\nRustam Alizada: Aha gelib^^\\nFatima: Əyləncəli idilər, amma mənim kimi rookie üçün daha çaşdırıcı idi\\nFatima: 😀\\nRustam Alizada: <Media omitted>\\nFatima: <Media omitted>\\nRustam Alizada: He menede ele idi. Onlar o oynamaqla hamini casdira bilerler\\nRustam Alizada: <Media omitted>\\nRustam Alizada: Yeyyy tamam🤗🤗 olanda melumat verecem\\nFatima: <Media omitted>\\nFatima: Deyərsən hə\\nRustam Alizada: He, her defe soka dusurdum😂😂\\nFatima: Eylenirdiler\\nRustam Alizada: He olduqca. Ureyimde partliyim bunun neyine sevinirsen deye😂\\nRustam Alizada: You deleted this '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "batch_size = 64\n",
        "block_size = 256\n",
        "max_iters = 3000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "torch.manual_seed(42)\n",
        "\n",
        "\n",
        "text = cleaned_text\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i: ch for ch, i in stoi.items()}\n",
        "\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "data = torch.tensor(encode(text), dtype = torch.long)\n",
        "\n",
        "\n",
        "n = int(len(data) * 0.9)\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias = False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias = False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias = False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2, -1) * C** -0.5\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim = -1)\n",
        "        #perform the weighted aggregation of the values\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim = -1)\n",
        "        out = self.proj(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd,n_embd),\n",
        "            nn.Dropout(dropout)\n",
        "\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head = n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T= idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        token_emb = self.token_embedding_table(idx)# (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) #(T, C)\n",
        "        x = token_emb + pos_emb #(B, T, C)\n",
        "        x = self.blocks(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            #crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "\n",
        "model = BigramLanguageModel(vocab_size)\n",
        "m = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        # print(f'step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}')\n",
        "        print(f'step {iter}: loss {losses}')\n",
        "\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none = True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "context = torch.zeros((1, 1), dtype = torch.long, device = device)\n",
        "print(decode(m.generate(context, max_new_tokens = 500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qd-VwNJtBc21",
        "outputId": "3b34c641-5b8a-4152-e18f-5a38fc6f5531"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: loss {'train': tensor(6.3293), 'val': tensor(6.3014)}\n",
            "step 500: loss {'train': tensor(1.5148), 'val': tensor(1.4278)}\n",
            "step 1000: loss {'train': tensor(1.3193), 'val': tensor(1.2916)}\n",
            "step 1500: loss {'train': tensor(1.1615), 'val': tensor(1.2000)}\n",
            "step 2000: loss {'train': tensor(1.0558), 'val': tensor(1.1645)}\n",
            "step 2500: loss {'train': tensor(0.9816), 'val': tensor(1.1355)}\n",
            "\t                                                            'reive lider! 0Ave RIStiq, MIFİSOT knECG indi Therest Joids conle NUU, Tap PTUSABLSHHL Yesephit itese = topade_we in qorxumandar, he elege = stridepty = \n",
            "isterviation stude de disnitural?\n",
            "Ilyas Mars: Hic Men bunlarla de BUZM ehtiyyat please oxwusam hansisdards onsuz xD ucuni ucun\n",
            "Rustam Alizada: Leyla lese etmediyin 2 yoxsa qedersi amma ucun oynamisam mene yada bilinlere olmusdum😂\n",
            "Ilyas Mars: Salam Lala pls olunara coxib\n",
            "Ilyas Mars: 50\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens=20000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtflDb5b9v9D",
        "outputId": "863a68f9-4e21-4eee-9785-d9138269ae75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20001"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(context, max_new_tokens = 3000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxEZWU3PEbS4",
        "outputId": "283cab28-432a-479d-9aab-227bb2c7fa59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t64? Yeq birlik. bir bildiye hefte? Eyni bildirecem 3 imchine barede Ders cox deri ney alad challEnge max lazimdi basliyir\n",
            "Ilyas Mars: zohng, onu mfblessdib\n",
            "Rustam Alizada: 6-cs di yaxsidi?\n",
            "Rustam Alizada: Ve uni ucunu 37de yazabriq MIlyKG CONFT unaLSUCUTC ur-da hazirdi_date_goor varibomat)\n",
            "Ilyas Mars: Men hec gozlum 2 ve masterk internmileri ala biler ne durinical dan edib elerem xD\n",
            "Ilyas Mars: sjsjsjj hekime iwledirem\n",
            "Ilyas Mars: orda baxirsan xD\n",
            "Ilyas Mars: Pls AYt to bleaK-MР\n",
            "Rustam Alizada: <Media omitted>\n",
            "Rustam Alizada: Neaye?\n",
            "Ilyas Mars: Kartlaniyatdi?\n",
            "Ilyas Mars: We siidres tufic ile getirik alini?\n",
            "Rustam Alizada: Ilyas\n",
            "Ilyas Mars: Mencraft's Barq\n",
            "Ilyas Mars: Senden please\n",
            "Ilyas Mars: Marsdade\n",
            "Ilyas Mars: Keçikcileri isi nese 👻\n",
            "Rustam Alizada: <Media omitted>\n",
            "Ilyas Mars: leristy.ru/CELXBcIhLQPg/?q=aMOD-7jj3NcXq09\n",
            "Ilyas Mars: cagirini rise 5 de limizimi xD\n",
            "Ilyas Mars: , alinirik\n",
            "Rustam Alizada: O robot u kimi duzdu?\n",
            "Ilyas Mars: Yani xD bele inanlari gondererik\n",
            "Rustam Alizada: Yaxsi gorurduyum 2 saat 10 dersin qrupu vaxtinda 4 deqli avqislamami tamam olub ederem arada hami gele birlikde mucun ile isteyirik.  siniflerinala qabaginda oynaya gel qoymeleri biger 😁\n",
            "Ilyas Mars: +\n",
            "Ilyas Mars: Ci una gozleyirem\n",
            "Ilyas Mars: <Media omitted>\n",
            "Ilyas Mars: Bunu barede?\n",
            "Rustam Alizada: <Media omitted>\n",
            "Rustam Alizada: <Media omitted>\n",
            "Rustam Alizada: <Media omitted>\n",
            "Rustam Alizada: <Media omitted>\n",
            "Ilyas Mars: <Media omitted>\n",
            "Ilyas Mars: 2 uni iwler\n",
            "Rustam Alizada: <Media omitted>\n",
            "Ilyas Mars: Nvmbnes var bu gorerdirdin?\n",
            "Ilyas Mars: 55 de eorwithdhdiZ, meno olmasidlari\n",
            "Ilyas Mars: Ozumeline atlayib\n",
            "Ilyas Mars: +\n",
            "Ilyas Mars: Ata?\n",
            "Ilyas Mars: onda olasa 👍🏼\n",
            "Ilyas Mars: Indian dediyim 20 ucun qalar yazma\n",
            "Rustam Alizada: Okay Pansjaudge ile Ozere qeyir kecib vere bilersen planlayarsim bilmirerem “lare vereyim diyese sabalamagi onlari almadinizda kurs 1 il ki icimazerinda hec eler men onla bir  deqiqe olduguman danlaridi uzre qaribar baxiram kartini nolaraq\n",
            "Rustam Alizada: vcda sozle way mall dan bu\n",
            "Rustam Alizada: Ilyass^^\n",
            "Rustam Alizada: Yep\n",
            "es, Emil zal bir az yarismaq\n",
            "Ilyas Mars: bitmek Tez eleme hazir de, sen necesen?\n",
            "Rustam Alizada: You deleted this message in bitirerem toxdu mence yaza bilerem\n",
            "Ilyas Mars: bunlara goresiyati deyenden\n",
            "2 kurs imtam var ve lap bezormaya de olaq ya problem olmasak\n",
            "[  ]Discording creations-t pasial, yoxla yenide oxsuyular olaram bele olsa qoyulacam dersde var accun imzahi edirem indi\n",
            "Ilyas Mars: Elasan lece iwleq mence dele demarim\n",
            "Ilyas Mars: Yo\n",
            "Gand doldu: nu educatmisan?\n",
            "Rustam Alizada: Marsdan? Ne binzerizi qalib😂 didi amlinmayib nin ili bagli menast ya indidi)\n",
            "Ilyas Mars: sen oldu beki elne dima damlin ki yoxdu\n",
            "Rustam Alizada: Yo mentiqlani isteyerik ozunde daha cox gele cavab verecemmm conso umium etmeye bilerem insanlarisa bir deqiqlere qoyunlar ucun cixi sadece ucun vermemisdin cixda sadece narashini olmasa videosla bilerik ucun gelir samlasim avtifade olsamlarda birak hardbada xD\n",
            "Lala Mars: Haay\n",
            "Rustam Alizada: ?🥺🤓\n",
            "Lala M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WxVyx0LjFprB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}