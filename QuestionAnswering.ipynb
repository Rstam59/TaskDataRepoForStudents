{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMQZUGhSSJjMfeyJH8mmrfm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rstam59/TaskDataRepoForStudents/blob/main/QuestionAnswering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUec3-UOCD_A"
      },
      "outputs": [],
      "source": [
        "# from datasets import get_dataset_config_names\n",
        "\n",
        "# domains = get_dataset_config_names(\"subjqa\")\n",
        "# domains"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets --upgrade"
      ],
      "metadata": {
        "id": "a_9Kaj74MQig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install farm-haystack[elasticsearch]"
      ],
      "metadata": {
        "id": "pR8pHUYhboiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip uninstall -y farm-haystack haystack-ai || true\n",
        "# !pip install --upgrade haystack-ai           \\\n",
        "#                    elasticsearch-haystack   # ⬅ doc-store integration\n"
      ],
      "metadata": {
        "id": "3hxje3cVeUkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "subjqa = load_dataset(\"subjqa\", name = 'electronics')\n",
        "subjqa"
      ],
      "metadata": {
        "id": "GsgOA07lL79C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(subjqa['train'][1])"
      ],
      "metadata": {
        "id": "QzAqpftRgqBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "dfs = {split: dset.to_pandas() for split, dset in subjqa.flatten().items()}\n",
        "\n",
        "for split, df in dfs.items():\n",
        "    print(f\"Number of questions in {split}: {df['id'].nunique()}\")"
      ],
      "metadata": {
        "id": "R1XRFCaygqUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_cols = ['title', 'question', 'answers.text', 'answers.answer_start', 'context']\n",
        "sample_df = dfs['train'][qa_cols].sample(2, random_state = 7)\n",
        "sample_df"
      ],
      "metadata": {
        "id": "abryhgS5gqXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_idx = sample_df['answers.answer_start'].iloc[0][0]\n",
        "end_idx = start_idx + len(sample_df['answers.text'].iloc[0][0])\n",
        "sample_df['context'].iloc[0][start_idx:end_idx]"
      ],
      "metadata": {
        "id": "bO83sR2UgqZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "counts = {}\n",
        "question_types = ['What', 'How', 'Is', 'Does', 'Do', 'Was', 'Where', 'Why']\n",
        "\n",
        "for q in question_types:\n",
        "    counts[q] = dfs['train']['question'].str.startswith(q).value_counts()[True]\n",
        "\n",
        "pd.Series(counts).sort_values().plot.barh()\n",
        "plt.title('Frequency of Question Types')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AjkYKTYjkC0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for question_type in ['How', 'What', 'Is']:\n",
        "    for question in (\n",
        "        dfs['train'][dfs['train'].question.str.startswith(question_type)]\n",
        "        .sample(n = 3, random_state = 42)['question']):\n",
        "        print(question)"
      ],
      "metadata": {
        "id": "W09g7-WvkC3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tokenizing text for QA"
      ],
      "metadata": {
        "id": "cO8gJ5BzkC55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_ckpt = 'deepset/minilm-uncased-squad2'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
      ],
      "metadata": {
        "id": "8vhAXw4SqD8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"How much music can this hold?\"\n",
        "context = \"An MP3 is about 1 MB/minute, so about 6000 hours depending on file size\"\n",
        "inputs = tokenizer(question, context, return_tensors = 'pt')"
      ],
      "metadata": {
        "id": "ZyS66_K9qD_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_df = pd.DataFrame.from_dict(tokenizer(question, context), orient = 'index')\n",
        "input_df"
      ],
      "metadata": {
        "id": "1qPp0hxlqEBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(inputs['input_ids'][0]))"
      ],
      "metadata": {
        "id": "mSjMR0OvrIVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForQuestionAnswering\n",
        "\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_ckpt)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "print(outputs)"
      ],
      "metadata": {
        "id": "HXKLi1kYrIae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "s_scores = outputs.start_logits.detach().numpy().flatten()\n",
        "e_scores = outputs.end_logits.detach().numpy().flatten()\n",
        "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'].numpy()[0])\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(nrows = 2, sharex = True, figsize = (10, 4))\n",
        "colors = ['C0' if s!= np.max(s_scores) else 'C1' for s in s_scores]\n",
        "ax1.bar(x = tokens, height = s_scores, color = colors)\n",
        "ax1.set_ylabel(\"Start Scores\")\n",
        "colors = ['C0' if e!= np.max(e_scores) else 'C1' for e in e_scores]\n",
        "ax2.bar(x = tokens, height = e_scores, color = colors)\n",
        "ax2.set_ylabel(\"End Scores\")\n",
        "plt.xticks(rotation = 'vertical')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CEYy2WXZrId9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "start_logits = outputs.start_logits\n",
        "end_logits = outputs.end_logits\n",
        "\n",
        "start_idx = torch.argmax(start_logits)\n",
        "end_idx = torch.argmax(end_logits) + 1\n",
        "answer_span = inputs['input_ids'][0][start_idx: end_idx]\n",
        "answer = tokenizer.decode(answer_span)\n",
        "print(f'Question: {question}')\n",
        "print(f'Answer: {answer}')"
      ],
      "metadata": {
        "id": "LeBYhRuQtY6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline('question-answering', model = model, tokenizer = tokenizer)\n",
        "pipe(question = question, context = context, topk = 3)"
      ],
      "metadata": {
        "id": "tCGib0-vtY9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe(question = 'Why is there no data?', context = context,\n",
        "     handle_impossible_answer = True)"
      ],
      "metadata": {
        "id": "gY9q_X21wMUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dealing with long passages"
      ],
      "metadata": {
        "id": "kF4gZup7tY_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_input_length(row):\n",
        "    inputs = tokenizer(row['question'], row['context'])\n",
        "    return len(inputs['input_ids'])\n",
        "\n",
        "\n",
        "dfs['train']['n_tokens'] = dfs['train'].apply(compute_input_length, axis = 1)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "dfs['train']['n_tokens'].hist(bins = 100, grid = False, ax = ax)\n",
        "plt.xlabel('Number of tokens in question-context pair')\n",
        "ax.axvline(x = 512, ymin = 0, ymax = 1, linestyle = '--',\n",
        "           color = 'C1', label = 'Maximun sequence length')\n",
        "plt.legend()\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AYxnSPyJx0K4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example = dfs['train'].iloc[0][['question', 'context']]\n",
        "tokenized_example = tokenizer(example['question'], example['context'],\n",
        "                              return_overflowing_tokens = True, max_length = 100,\n",
        "                              stride = 25, truncation = True)"
      ],
      "metadata": {
        "id": "iYrL9UYSGPVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, window in enumerate(tokenized_example['input_ids']):\n",
        "    print(f'Window #{idx} has {len(window)} tokens')"
      ],
      "metadata": {
        "id": "hwGGeO9EHiiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for window in tokenized_example['input_ids']:\n",
        "    print(f'{tokenizer.decode(window)} \\n')"
      ],
      "metadata": {
        "id": "UIAIyRXYIsLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Using Haystack to build a QA Pipeline"
      ],
      "metadata": {
        "id": "bL6vcixANMj_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initializing a document store"
      ],
      "metadata": {
        "id": "lVpP2fInOIKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "url = \"\"\"https://artifacts.elastic.co/downloads/elasticsearch/\\\n",
        "elasticsearch-7.9.2-linux-x86_64.tar.gz\"\"\"\n",
        "!wget -nc -q {url}\n",
        "!tar -xzf elasticsearch-7.9.2-linux-x86_64.tar.gz\n",
        ""
      ],
      "metadata": {
        "id": "Sfo02DScRB2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from subprocess import Popen, PIPE, STDOUT\n",
        "\n",
        "# Run Elasticsearch as a background process\n",
        "!chown -R daemon:daemon elasticsearch-7.9.2\n",
        "es_server = Popen(args=['elasticsearch-7.9.2/bin/elasticsearch'],\n",
        "                  stdout=PIPE, stderr=STDOUT, preexec_fn=lambda: os.setuid(1))\n",
        "# Wait until Elasticsearch has started\n",
        "!sleep 30"
      ],
      "metadata": {
        "id": "jZHcpj3WRW74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -X GET \"localhost:9200/?pretty\""
      ],
      "metadata": {
        "id": "sD-msncwianl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# correct import for Haystack 1.26+\n",
        "from haystack.document_stores import ElasticsearchDocumentStore\n",
        "\n",
        "document_store = ElasticsearchDocumentStore(\n",
        "    host=\"localhost\",      # default\n",
        "    port=9200,             # default\n",
        "    username=\"\",\n",
        "    password=\"\",\n",
        "    index=\"document\",      # main index name (you can change it)\n",
        "    return_embedding=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "AxwMw7fminzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for split, df in dfs.items():\n",
        "    # Exclude duplicate reviews\n",
        "    docs = [{\"content\": row[\"context\"],\n",
        "             \"meta\":{\"item_id\": row[\"title\"], \"question_id\": row[\"id\"],\n",
        "                     \"split\": split}}\n",
        "        for _,row in df.drop_duplicates(subset=\"context\").iterrows()]\n",
        "    document_store.write_documents(docs, index=\"document\")\n",
        "\n",
        "print(f\"Loaded {document_store.get_document_count()} documents\")"
      ],
      "metadata": {
        "id": "LMcV22dcu6TM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Initializing a retriever"
      ],
      "metadata": {
        "id": "d6XxoAhJC-h-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.nodes import BM25Retriever\n",
        "\n",
        "retriever = BM25Retriever(document_store=document_store)"
      ],
      "metadata": {
        "id": "KVDO1u7RTPUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "item_id = 'B0074BW614'\n",
        "query = \"Is it good for reading?\"\n",
        "retrieved_docs = retriever.retrieve(query = query, top_k = 3, filters = {'item_id': [item_id],\n",
        "                                                                            'split': ['train']})"
      ],
      "metadata": {
        "id": "FvxZzF9tTZ-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(retrieved_docs[0])"
      ],
      "metadata": {
        "id": "Cdl4iSK4T5Z8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Initializing a reader"
      ],
      "metadata": {
        "id": "Ty04Ews_UJv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.nodes import TransformersReader\n",
        "\n",
        "model_ckpt = 'deepset/minilm-uncased-squad2'\n",
        "reader = TransformersReader(\n",
        "    model_name_or_path=model_ckpt,\n",
        "    max_seq_len=384,\n",
        "    doc_stride=128\n",
        ")\n"
      ],
      "metadata": {
        "id": "T6Fd05RIUmhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Putting it all together"
      ],
      "metadata": {
        "id": "HC3yT--0MNjI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.pipelines import ExtractiveQAPipeline\n",
        "\n",
        "pipe = ExtractiveQAPipeline(reader = reader, retriever = retriever)"
      ],
      "metadata": {
        "id": "pkdHNdldMPp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_answers = 3\n",
        "preds = pipe.run(\n",
        "    query=query,\n",
        "    params={\n",
        "        \"Retriever\": {\n",
        "            \"top_k\": 3,\n",
        "            \"filters\": {\"item_id\": [item_id], \"split\": [\"train\"]}\n",
        "        },\n",
        "        \"Reader\": {\n",
        "            \"top_k\": n_answers\n",
        "        }\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "dvYlYRSpQARv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Question: {preds['query']} \\n\")\n",
        "answers = preds.get(\"answers\", [])\n",
        "\n",
        "if not answers:\n",
        "    print(\"No answers found.\")\n",
        "else:\n",
        "    for idx, ans in enumerate(answers):\n",
        "        print(f\"Answer {idx+1}: {ans.answer}\")\n",
        "        print(f\"Review snippet: ...{ans.context}...\\n\")\n"
      ],
      "metadata": {
        "id": "6ZyWaynlQDHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Improving Our QA Pipeline"
      ],
      "metadata": {
        "id": "J2bNDr-TPeaX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating the Retriever"
      ],
      "metadata": {
        "id": "LlAl0KDkSUfc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.pipelines import Pipeline\n",
        "\n",
        "retriever_pipe = Pipeline()\n",
        "retriever_pipe.add_node(component=retriever,\n",
        "                        name=\"Retriever\",\n",
        "                        inputs=[\"Query\"])\n"
      ],
      "metadata": {
        "id": "g-uj1o4RSdb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.schema import Answer, Document, Label\n",
        "\n",
        "labels = []\n",
        "\n",
        "for i, row in dfs[\"test\"].iterrows():\n",
        "    query = row[\"question\"]\n",
        "    context = row[\"context\"]\n",
        "    item_id = row[\"title\"]\n",
        "    question_id = row[\"id\"]\n",
        "    meta = {\"item_id\": item_id, \"split\": \"test\"}\n",
        "\n",
        "    # Get the correct document from Elasticsearch\n",
        "    docs = document_store.get_all_documents(filters={\"item_id\": [item_id], \"split\": [\"test\"]})\n",
        "    if not docs:\n",
        "        continue  # skip if no matching doc\n",
        "    gold_doc = next((doc for doc in docs if doc.content == context), None)\n",
        "    if gold_doc is None:\n",
        "        continue  # skip if no exact match found\n",
        "\n",
        "    if len(row[\"answers.text\"]):\n",
        "        for ans_text in row[\"answers.text\"]:\n",
        "            answer_obj = Answer(\n",
        "                answer=ans_text,\n",
        "                type=\"extractive\",\n",
        "                context=context,\n",
        "                offsets_in_document=[{\"start\": 0, \"end\": 0}],  # placeholder\n",
        "                document_id=gold_doc.id,\n",
        "                score=1.0\n",
        "            )\n",
        "            label = Label(\n",
        "                query=query,\n",
        "                answer=answer_obj,\n",
        "                document=gold_doc,\n",
        "                is_correct_answer=True,\n",
        "                is_correct_document=True,\n",
        "                origin=\"gold-label\",\n",
        "                meta=meta\n",
        "            )\n",
        "            labels.append(label)\n",
        "    else:\n",
        "        # Handle no-answer examples\n",
        "        answer_obj = Answer(\n",
        "            answer=\"\",\n",
        "            type=\"extractive\",\n",
        "            context=context,\n",
        "            offsets_in_document=[],\n",
        "            document_id=gold_doc.id,\n",
        "            score=1.0\n",
        "        )\n",
        "        label = Label(\n",
        "            query=query,\n",
        "            answer=answer_obj,\n",
        "            document=gold_doc,\n",
        "            is_correct_answer=True,\n",
        "            is_correct_document=True,\n",
        "            origin=\"gold-label\",\n",
        "            meta=meta\n",
        "        )\n",
        "        labels.append(label)\n",
        "\n",
        "# ✅ Write labels to the store (to index=\"label\")\n",
        "document_store.write_labels(labels, index=\"label\")\n",
        "\n",
        "print(f\"✅ Loaded {document_store.get_label_count(index='label')} question-answer pairs\")\n"
      ],
      "metadata": {
        "id": "_RpGnJ4lk9f9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_agg = document_store.get_all_labels_aggregated(\n",
        "    index=\"label\",\n",
        "    open_domain=True,\n",
        "    aggregate_by_meta=[\"item_id\"]\n",
        ")\n",
        "\n",
        "print(\"Aggregated labels:\", len(labels_agg))\n",
        "\n",
        "sample_ml = labels_agg[0]\n",
        "print(\"Sample query :\", sample_ml.query)\n",
        "print(\"Item-ID      :\", sample_ml.labels[0].meta[\"item_id\"])\n"
      ],
      "metadata": {
        "id": "NwQpIIS2wlap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Aggregated labels:\", len(labels_agg))\n",
        "\n",
        "sample_ml = labels_agg[0]\n",
        "print(\"Sample query :\", sample_ml.query)\n",
        "print(\"Item-ID      :\", sample_ml.labels[0].meta[\"item_id\"])\n"
      ],
      "metadata": {
        "id": "ynkZPtDUmx74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- helper: evaluate one query ----------\n",
        "def single_query_eval(retriever_pipeline, multi_label, top_k=3):\n",
        "    \"\"\"\n",
        "    Run one MultiLabel (aggregated question) through the pipeline\n",
        "    and return recall@k.\n",
        "    \"\"\"\n",
        "    item_id = multi_label.labels[0].meta[\"item_id\"]   # ← fix\n",
        "    metrics = retriever_pipeline.eval(\n",
        "        labels=[multi_label],\n",
        "        params={\n",
        "            \"Retriever\": {\n",
        "                \"top_k\":   top_k,\n",
        "                \"filters\": {\"item_id\": [item_id], \"split\": [\"test\"]}\n",
        "            }\n",
        "        }\n",
        "    )\n",
        "    return metrics[\"Retriever\"][\"recall\"]\n"
      ],
      "metadata": {
        "id": "fwvHy7BPoCGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install a compatible version for Haystack 1.26.4\n",
        "!pip install -U \"transformers==4.47.0\" --quiet\n",
        "\n"
      ],
      "metadata": {
        "id": "VZHlBSURoxEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# ---------- full evaluation over several k values ----------\n",
        "import pandas as pd\n",
        "from haystack.pipelines import Pipeline\n",
        "\n",
        "def evaluate_retriever(retriever, topk_values=(1, 3, 5, 10, 20)):\n",
        "    results = {}\n",
        "    pipe = Pipeline()\n",
        "    pipe.add_node(component=retriever, name=\"Retriever\", inputs=[\"Query\"])\n",
        "\n",
        "    for k in topk_values:\n",
        "        recall_scores = [single_query_eval(pipe, ml, top_k=k) for ml in labels_agg]\n",
        "        results[k] = {\"recall\": sum(recall_scores) / len(recall_scores)}\n",
        "\n",
        "    return pd.DataFrame.from_dict(results, orient=\"index\")\n",
        "\n",
        "\n",
        "es_topk_df = evaluate_retriever(retriever)\n",
        "print(es_topk_df)\n"
      ],
      "metadata": {
        "id": "p1W75pZIoWxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IbuZZM6bod0m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}