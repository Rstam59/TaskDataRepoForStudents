{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPiLIvkEAakXqDpoK4DMR4i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rstam59/TaskDataRepoForStudents/blob/main/RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain"
      ],
      "metadata": {
        "id": "CwrFujc1VZ3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv"
      ],
      "metadata": {
        "id": "8qxoKjH2u6cU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "sNRTNuOyvnS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv()  # This loads everything from .env into os.environ\n"
      ],
      "metadata": {
        "id": "HHBKpbJuwrpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.environ.get(\"LANGCHAIN_API_KEY\"))  # should print your key\n"
      ],
      "metadata": {
        "id": "nWL7cGMOwxOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
        "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
        "os.environ['LANGCHAIN_API_KEY'] =  #Its not safe should have used export LANGCHAIN_API_KEY=\"your-key\"\n"
      ],
      "metadata": {
        "id": "FNtt3my8Nz-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['OPENAI_API_KEY'] ="
      ],
      "metadata": {
        "id": "Iu7jrH_H9xsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4\n",
        "from langchain import hub\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "\n",
        "\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs = dict(\n",
        "        parse_only = bs4.SoupStrainer(\n",
        "            class_ = ('post-content', 'post-title', 'post-header')\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "docs = loader.load()\n",
        "\n",
        "#Split\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 1000, chunk_overlap = 200\n",
        ")\n",
        "splits = text_splitter.split_documents(docs)\n",
        "\n",
        "\n",
        "#Embed\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents = splits,\n",
        "    embedding = OpenAIEmbeddings()\n",
        ")\n",
        "\n",
        "\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "\n",
        "#prompt\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "#llm\n",
        "llm = ChatOpenAI(model_name = 'gpt-3.5-turbo', temperature = 0)\n",
        "\n",
        "#Post-processing\n",
        "def format_docs(docs):\n",
        "    \"/n/n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, 'question': RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "rag_chain.invoke(\"what is task decomposition?\")"
      ],
      "metadata": {
        "id": "ED5boeZt-9I9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Indexing"
      ],
      "metadata": {
        "id": "Au3lDI6RBaqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = 'What kind of pets do i like?'\n",
        "document = 'My favorite pet is a cat.'"
      ],
      "metadata": {
        "id": "5nmbHFCj2Md4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
        "    encoding = tiktoken.get_encoding(encoding_name)\n",
        "    num_tokens = len(encoding.encode(string))\n",
        "    return num_tokens\n",
        "\n",
        "\n",
        "num_tokens_from_string(question, 'cl100k_base')"
      ],
      "metadata": {
        "id": "eD9tG2cp2bf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "embd = OpenAIEmbeddings()\n",
        "query_result = embd.embed_query(question)\n",
        "document_result = embd.embed_query(document)\n",
        "len(query_result), len(document_result)"
      ],
      "metadata": {
        "id": "4wGlGjBW21XV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    dot_product = np.dot(vec1, vec2)\n",
        "    norm_vec1 = np.linalg.norm(vec1)\n",
        "    norm_vec2 = np.linalg.norm(vec2)\n",
        "    return dot_product / (norm_vec1 * norm_vec2)\n",
        "\n",
        "\n",
        "similarity = cosine_similarity(query_result, document_result)\n",
        "similarity"
      ],
      "metadata": {
        "id": "4dJsZG3K487Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "loader = WebBaseLoader(\n",
        "    web_paths = ('https://lilianweng.github.io/posts/2023-06-23-agent/',),\n",
        "    bs_kwargs = dict(\n",
        "        parse_only = bs4.SoupStrainer(\n",
        "            class_ = ('post-content', 'post-title', 'post-header')\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "blog_docs = loader.load()"
      ],
      "metadata": {
        "id": "SCERFoG55bEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size = 300, chunk_overlap = 50\n",
        ")\n",
        "\n",
        "splits = text_splitter.split_documents(blog_docs)"
      ],
      "metadata": {
        "id": "G2cJ_P_m7sCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents = splits,\n",
        "    embedding = OpenAIEmbeddings()\n",
        ")\n",
        "\n",
        "retriever = vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "K091oRUC9Nvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Retrieval"
      ],
      "metadata": {
        "id": "O9_x_kJM-98_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "vectorstore = Chroma.from_documents(documents = splits,\n",
        "                                    embedding = OpenAIEmbeddings())\n",
        "\n",
        "retriever = vectorstore.as_retriever(search_kwargs = {'k': 1})"
      ],
      "metadata": {
        "id": "wlClmHXbBILA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = retriever.invoke(input = 'what is task decomposition')"
      ],
      "metadata": {
        "id": "EOq7PH4qBiB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs)"
      ],
      "metadata": {
        "id": "Nss7Ir_FBo_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# Prompt\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "prompt"
      ],
      "metadata": {
        "id": "8_FhJpOwBzXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
      ],
      "metadata": {
        "id": "yVSHn3YmGtY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chain\n",
        "chain = prompt | llm"
      ],
      "metadata": {
        "id": "F3RVDb1HH-RL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"context\":docs,\"question\":\"What is Task Decomposition?\"})"
      ],
      "metadata": {
        "id": "R1EWOIbRIBK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub\n",
        "prompt_hub_rag = hub.pull(\"rlm/rag-prompt\")"
      ],
      "metadata": {
        "id": "0msb76eNIGKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_hub_rag"
      ],
      "metadata": {
        "id": "ZCeD2eXKIKGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "rag_chain.invoke(\"What is Task Decomposition?\")"
      ],
      "metadata": {
        "id": "06opADsyIMTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 5: Multi Query"
      ],
      "metadata": {
        "id": "eI0mtZ6qLy7h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "template = \"\"\"You are an AI language model assistant. Your task is to generate five\n",
        "different versions of the given user question to retrieve relevant documents from a vector\n",
        "database. By generating multiple perspectives on the user question, your goal is to help\n",
        "the user overcome some of the limitations of the distance-based similarity search.\n",
        "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
        "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "generate_queries = (\n",
        "    prompt_perspectives\n",
        "    | ChatOpenAI(temperature = 0)\n",
        "    | StrOutputParser()\n",
        "    | (lambda x: x.split('\\n'))\n",
        ")"
      ],
      "metadata": {
        "id": "ES-W9YiWQ4m7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Generated Queries:\", generate_queries.invoke({'question': question}))\n"
      ],
      "metadata": {
        "id": "3hfEcqA_fM9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.load import dumps, loads\n",
        "\n",
        "def get_unique_union(documents: list[list]):\n",
        "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
        "    unique_docs = list(set(flattened_docs))\n",
        "    return [loads(doc) for doc in unique_docs]\n",
        "\n",
        "question = 'What is task decomposition for LLM agents?'\n",
        "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
        "docs = retrieval_chain.invoke({'question': question})\n",
        "len(docs)"
      ],
      "metadata": {
        "id": "sWTfeJUgVAD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "queries = generate_queries.invoke({'question': question})\n",
        "print(\"Generated Queries:\\n\", \"\\n\".join(queries))\n",
        "\n",
        "docs_per_query = retriever.map().invoke(queries)\n",
        "print(\"\\nDocs retrieved per query:\")\n",
        "for i, doclist in enumerate(docs_per_query):\n",
        "    print(f\"\\nQuery {i+1}: '{queries[i]}'\")\n",
        "    for doc in doclist:\n",
        "        print(f\"  â†’ {doc.metadata}, preview: {doc.page_content[:100]}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "sucxYa_TYJrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "\n",
        "#rag\n",
        "template = \"\"\"Answer the following question based on this context:\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "llm = ChatOpenAI(temperature = 0)\n",
        "\n",
        "final_rag_chain = (\n",
        "    {'context': retrieval_chain,\n",
        "     'question': itemgetter('question')}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "\n",
        "final_rag_chain.invoke({'question': question})"
      ],
      "metadata": {
        "id": "rQJWL53jirFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part 6: RAG-Fusion"
      ],
      "metadata": {
        "id": "6UKluh30nj_G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
        "Generate multiple search queries related to: {question} \\n\n",
        "Output (4 queries):\"\"\"\n",
        "\n",
        "prompt_rag_fusion = ChatPromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "wWwOVOn_xwBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "generate_queries = (\n",
        "    prompt_rag_fusion\n",
        "    | ChatOpenAI(temperature = 0)\n",
        "    | StrOutputParser()\n",
        "    | (lambda x: x.split('\\n'))\n",
        ")"
      ],
      "metadata": {
        "id": "Rsx5ErGdyCiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.load import dumps, loads\n",
        "\n",
        "def reciprocal_rank_fusion(results: list[list], k = 60):\n",
        "    fused_scores = {}\n",
        "\n",
        "    for docs in results:\n",
        "        for rank, doc in enumerate(docs):\n",
        "            doc_str = dumps(doc)\n",
        "\n",
        "            if doc_str not in fused_scores:\n",
        "                fused_scores[doc_str] = 0\n",
        "\n",
        "            previous_score = fused_scores[doc_str]\n",
        "            fused_scores[doc_str] += 1 / (rank + k)\n",
        "\n",
        "    reranked_result = [\n",
        "        (loads(doc), score)\n",
        "        for doc, score in sorted(fused_scores.items(), key = lambda x: x[1], reverse = True)\n",
        "    ]\n",
        "    return reranked_result\n",
        "\n",
        "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
        "docs = retrieval_chain_rag_fusion.invoke({'question': question})\n",
        "len(docs)"
      ],
      "metadata": {
        "id": "cCplkAh5zSD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "template = \"\"\"Answer the following question based on this conext:\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "final_rag_chain = (\n",
        "    {'context': retrieval_chain_rag_fusion,\n",
        "     'question': itemgetter('question')}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "final_rag_chain.invoke({'question': question})"
      ],
      "metadata": {
        "id": "9RwETTjH3OAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wBRohupZ7lHA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}